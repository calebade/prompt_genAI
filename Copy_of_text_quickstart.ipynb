{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/calebade/prompt_genAI/blob/main/Copy_of_text_quickstart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tce3stUlHN0L"
      },
      "source": [
        "##### Copyright 2023 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "tuOe1ymfHZPu"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeadDkMiISin"
      },
      "source": [
        "# PaLM API: Text Quickstart with Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEXQ3OwKIa-O"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://developers.generativeai.google/tutorials/text_quickstart\"><img src=\"https://developers.generativeai.google/static/site-assets/images/docs/notebook-site-button.png\" height=\"32\" width=\"32\" />View on Generative AI</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/tutorials/text_quickstart.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/google/generative-ai-docs/blob/main/site/en/tutorials/text_quickstart.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp_CKyzxUqx6"
      },
      "source": [
        "In this notebook, you'll learn how to get started with the PaLM API, which gives you access to Google's latest large language models. Here, you'll learn how to use the PaLM API's text generation features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQnbmB70zqon"
      },
      "source": [
        "## Setup\n",
        "\n",
        "**Note**: At this time, the PaLM API is only available in the US."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gePcZFDdypdP"
      },
      "source": [
        "First, download and install the PaLM API Python library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Mk2d90cCdF4u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eee8a0ad-7115-424d-99c4-a9c3a8632eb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/122.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m122.9/122.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/122.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/113.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.3/113.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zCm_GdCKYDNm"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "import google.generativeai as palm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taFqmFg-7_PY"
      },
      "source": [
        "### Grab an API Key\n",
        "\n",
        "To get started, you'll need to [create an API key](https://developers.generativeai.google/tutorials/setup)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YE1x5qv-hka3"
      },
      "outputs": [],
      "source": [
        "palm.configure(api_key='AIzaSyDQ62_mzLnCWXpBh-ETbqhqov_AOjaRNK0')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2dk6P3nJz6m"
      },
      "source": [
        "## Text generation\n",
        "\n",
        "Use the `palm.list_models` function to find available models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dKLCxpTw_C2Q",
        "outputId": "271ee554-eda6-446f-977c-f7f9e04cc6dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/text-bison-001\n"
          ]
        }
      ],
      "source": [
        "models = [m for m in palm.list_models() if 'generateText' in m.supported_generation_methods]\n",
        "model = models[0].name\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OvvPDyVvb4X"
      },
      "source": [
        "Use the `palm.generate_text` method to generate text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "sJm90x7J6YLe",
        "outputId": "ebda3098-bada-483b-8011-5c8c1c30236a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chain-of-thought:\n",
            "First find the total number of cats: 3 houses * 3 cats / house = 9 cats. Then multiply the number of cats by the number of mittens per cat to find the total number of mittens: 9 cats * 4 mittens / cat = 36 mittens. Then multiply the number of mittens by the length of yarn per mitten to find the total length of yarn used for mittens: 36 mittens * 7m / mitten = 252m. Then multiply the number of cats by the number of hats per cat to find the total number of hats: 9 cats * 1 hat / cat = 9 hats. Then multiply the number of hats by the length of yarn per hat to find the total length of yarn used for hats: 9 hats * 4m / hat = 36m. Then add the length of yarn used for mittens and hats to find the total length of yarn used: 252m + 36m = 288m.\n",
            "\n",
            "The answer should be 288\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "You are an expert at solving word problems.\n",
        "\n",
        "Solve the following problem:\n",
        "\n",
        "I have three houses, each with three cats.\n",
        "each cat owns 4 mittens, and a hat. Each mitten was\n",
        "knit from 7m of yarn, each hat from 4m.\n",
        "How much yarn was needed to make all the items?\n",
        "\n",
        "Think about it step by step, and show your work.\n",
        "\"\"\"\n",
        "\n",
        "completion = palm.generate_text(\n",
        "    model=model,\n",
        "    prompt=prompt,\n",
        "    temperature=0,\n",
        "    # The maximum length of the response\n",
        "    max_output_tokens=800,\n",
        ")\n",
        "\n",
        "print(completion.result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eca9b28591a"
      },
      "source": [
        "## More options\n",
        "\n",
        "The `palm.generate_text` function has a few other arguments worth mentioning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dede032595a0"
      },
      "source": [
        "### Stop sequences\n",
        "\n",
        "Use the `stop_sequences` argument to stop generation early.\n",
        "\n",
        "For example LLM's often make mistakes in arithmetic. You could ask the model to \"use a calculator\" by putting equations in a `<calc>` tag.\n",
        "\n",
        "Have the model stop at the closing tag, so you can edit the prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ecc8e5092861"
      },
      "outputs": [],
      "source": [
        "calc_prompt = f\"\"\"\n",
        "Please solve the following problem.\n",
        "\n",
        "{prompt}\n",
        "\n",
        "----------------\n",
        "\n",
        "Important: Use the calculator for each step.\n",
        "Don't do the arithmetic in your head.\n",
        "\n",
        "To use the calculator wrap an equation in <calc> tags like this:\n",
        "\n",
        "<calc> 3 cats * 2 hats/cat </calc> = 6\n",
        "\n",
        "----------------\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "equation=None\n",
        "while equation is None:\n",
        "    completion = palm.generate_text(\n",
        "        model=model,\n",
        "        prompt=calc_prompt,\n",
        "        stop_sequences=['</calc>'],\n",
        "        # The maximum length of the response\n",
        "        max_output_tokens=800,\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response, equation = completion.result.split('<calc>', maxsplit=1)\n",
        "    except Exception:\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "21cd8789a136",
        "outputId": "147c4cd6-74b3-4b86-cff0-33854ab9070c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. There are 3 houses * 3 cats / house = 9 cats.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "6f23f57698f2",
        "outputId": "d8efc873-096d-4e68-cf28-fea3816c86be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 3 houses * 3 cats / house \n"
          ]
        }
      ],
      "source": [
        "print(equation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13eda23afe44"
      },
      "source": [
        "From there you can calculate the result, and assemble a new prompt for the model to continue from.\n",
        "For a complete working implementation see the [Text calculator example](../examples/text_calculator.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1b0509ef19e"
      },
      "source": [
        "### Candidates\n",
        "\n",
        "Typically, there's some degree of randomness in the text produced by LLMs. (Read more about why in the [LLM primer](https://developers.generativeai.google/guide/concepts)). That means that when you call the API more than once with the same input, you might get different responses. You can use this feature to your advantage to get alternate model responses.\n",
        "\n",
        "The `temperature` argument controls the variance of the responses. The `palm.Model` object gives the default value for `temperature` and other parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "s6doS7vAvxz8",
        "outputId": "6b9e1725-c1d3-42f2-f1fe-7ca2ce6bfecf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(name='models/text-bison-001', base_model_id='', version='001', display_name='Text Bison', description='Model targeted for text generation.', input_token_limit=8196, output_token_limit=1024, supported_generation_methods=['generateText', 'countTextTokens'], temperature=0.7, top_p=0.95, top_k=40)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "models[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYJbWwzXwXiD"
      },
      "source": [
        "The `candidate_count` argument controls the number of responses returned:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "NCvc78t-ROrL",
        "outputId": "70e61ec1-96bc-4348-b10a-89a4a423e65f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3 houses * 3 cats / house = 9 cats\n",
            "9 cats * 4 mittens / cat = 36 mittens\n",
            "9 cats * 1 hat / cat = 9 hats\n",
            "36 mittens * 7m / mitten = 252m\n",
            "9 hats * 4m / hat = 36m\n",
            "252m + 36m = 288m\n",
            "Therefore, 288m of yarn was needed to make all the items.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "completion = palm.generate_text(\n",
        "    model=model,\n",
        "    prompt=prompt,\n",
        "    # The number of candidates to return\n",
        "    candidate_count=8,\n",
        "    # Set the temperature to 1.0 for more variety of responses.\n",
        "    temperature=1.0,\n",
        "    max_output_tokens=800,\n",
        ")\n",
        "\n",
        "print(completion.result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lXu8iu0Mm3V"
      },
      "source": [
        "When you request multiple candidates the `Completion.result` attribute still just contains the first one. The `Completion.candidates` attribute contains all of them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "nJEjPHGVMPaj",
        "outputId": "77a0b3eb-60d3-4d63-dbb0-924440881f2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'output': '3 houses * 3 cats / house = 9 cats\\n'\n",
            "            '9 cats * 4 mittens / cat = 36 mittens\\n'\n",
            "            '9 cats * 1 hat / cat = 9 hats\\n'\n",
            "            '36 mittens * 7m / mitten = 252m\\n'\n",
            "            '9 hats * 4m / hat = 36m\\n'\n",
            "            '252m + 36m = 288m\\n'\n",
            "            'Therefore, 288m of yarn was needed to make all the items.\\n',\n",
            "  'safety_ratings': [{'category': <HarmCategory.HARM_CATEGORY_DEROGATORY: 1>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>},\n",
            "                     {'category': <HarmCategory.HARM_CATEGORY_TOXICITY: 2>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>},\n",
            "                     {'category': <HarmCategory.HARM_CATEGORY_VIOLENCE: 3>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>},\n",
            "                     {'category': <HarmCategory.HARM_CATEGORY_SEXUAL: 4>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>},\n",
            "                     {'category': <HarmCategory.HARM_CATEGORY_MEDICAL: 5>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>},\n",
            "                     {'category': <HarmCategory.HARM_CATEGORY_DANGEROUS: 6>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>}]},\n",
            " {'output': '1 cat has 4 mittens + 1 hat = 5 items. 3 cats have 5 items / cat '\n",
            "            '* 3 cats = 15 items. 15 items * 7m / item = 105m of yarn for '\n",
            "            'mittens. 15 items * 4m / item = 60m of yarn for hats. 105m + 60m '\n",
            "            '= 165m of yarn was needed to make the items.\\n'\n",
            "            '\\n'\n",
            "            'The answer: 165',\n",
            "  'safety_ratings': [{'category': <HarmCategory.HARM_CATEGORY_DEROGATORY: 1>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>},\n",
            "                     {'category': <HarmCategory.HARM_CATEGORY_TOXICITY: 2>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>},\n",
            "                     {'category': <HarmCategory.HARM_CATEGORY_VIOLENCE: 3>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>},\n",
            "                     {'category': <HarmCategory.HARM_CATEGORY_SEXUAL: 4>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>},\n",
            "                     {'category': <HarmCategory.HARM_CATEGORY_MEDICAL: 5>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>},\n",
            "                     {'category': <HarmCategory.HARM_CATEGORY_DANGEROUS: 6>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>}]},\n",
            " {'output': '3 houses * 3 cats / house * 4 mittens / cat * 7m / mitten = 108m '\n",
            "            'of yarn for mittens.\\n'\n",
            "            '3 houses * 3 cats / house * 1 hat / cat * 4m / hat = 36m of yarn '\n",
            "            'for hats.\\n'\n",
            "            '108m + 36m = 144m of yarn.\\n'\n",
            "            '\\n'\n",
            "            'The final answer: 144.',\n",
            "  'safety_ratings': [{'category': <HarmCategory.HARM_CATEGORY_DEROGATORY: 1>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>},\n",
            "                     {'category': <HarmCategory.HARM_CATEGORY_TOXICITY: 2>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>},\n",
            "                     {'category': <HarmCategory.HARM_CATEGORY_VIOLENCE: 3>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>},\n",
            "                     {'category': <HarmCategory.HARM_CATEGORY_SEXUAL: 4>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>},\n",
            "                     {'category': <HarmCategory.HARM_CATEGORY_MEDICAL: 5>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>},\n",
            "                     {'category': <HarmCategory.HARM_CATEGORY_DANGEROUS: 6>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>}]},\n",
            " {'output': '3 houses * 3 cats / house * 4 mittens / cat = 36 mittens.\\n'\n",
            "            '36 mittens * 7m / mitten = 252m yarn for mittens.\\n'\n",
            "            '3 houses * 3 cats / house * 1 hat / cat = 9 hats.\\n'\n",
            "            '9 hats * 4m / hat = 36m yarn for hats.\\n'\n",
            "            '252m yarn for mittens + 36m yarn for hats = 288m yarn.\\n'\n",
            "            '\\n'\n",
            "            'Thus, the answer is 288.',\n",
            "  'safety_ratings': [{'category': <HarmCategory.HARM_CATEGORY_DEROGATORY: 1>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>},\n",
            "                     {'category': <HarmCategory.HARM_CATEGORY_TOXICITY: 2>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>},\n",
            "                     {'category': <HarmCategory.HARM_CATEGORY_VIOLENCE: 3>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>},\n",
            "                     {'category': <HarmCategory.HARM_CATEGORY_SEXUAL: 4>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>},\n",
            "                     {'category': <HarmCategory.HARM_CATEGORY_MEDICAL: 5>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>},\n",
            "                     {'category': <HarmCategory.HARM_CATEGORY_DANGEROUS: 6>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>}]},\n",
            " {'output': 'First find the total number of cats: 3 houses * 3 cats / house = '\n",
            "            '9 cats. Then multiply that number by the number of mittens each '\n",
            "            'cat owns: 9 cats * 4 mittens / cat = 36 mittens. Then multiply '\n",
            "            'that number by the length of yarn per mitten to find the total '\n",
            "            'length of yarn used for mittens: 36 mittens * 7 m / mitten = 252 '\n",
            "            'm. Then do the same for hats: 9 cats * 1 hat / cat = 9 hats. Then '\n",
            "            'multiply that number by the length of yarn per hat to find the '\n",
            "            'total length of yarn used for hats: 9 hats * 4 m / hat = 36 m. '\n",
            "            'Then add the lengths of yarn used for mittens and hats to find '\n",
            "            'the total length of yarn used: 252 m + 36 m = 288 m.\\n'\n",
            "            '\\n'\n",
            "            'The answer: 288',\n",
            "  'safety_ratings': [{'category': <HarmCategory.HARM_CATEGORY_DEROGATORY: 1>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>},\n",
            "                     {'category': <HarmCategory.HARM_CATEGORY_TOXICITY: 2>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>},\n",
            "                     {'category': <HarmCategory.HARM_CATEGORY_VIOLENCE: 3>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>},\n",
            "                     {'category': <HarmCategory.HARM_CATEGORY_SEXUAL: 4>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>},\n",
            "                     {'category': <HarmCategory.HARM_CATEGORY_MEDICAL: 5>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>},\n",
            "                     {'category': <HarmCategory.HARM_CATEGORY_DANGEROUS: 6>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>}]},\n",
            " {'output': 'There are 3 houses * 3 cats / house = 9 cats. 9 cats * 4 mittens '\n",
            "            '/ cat = 36 mittens. 36 mittens * 7 m / mitten = 252 m of yarn for '\n",
            "            'mittens. 9 cats * 1 hat / cat = 9 hats. 9 hats * 4 m / hat = 36 m '\n",
            "            'of yarn for hats. 252 m + 36 m = 288 m of yarn.\\n'\n",
            "            'Thus, the answer is 288.',\n",
            "  'safety_ratings': [{'category': <HarmCategory.HARM_CATEGORY_DEROGATORY: 1>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>},\n",
            "                     {'category': <HarmCategory.HARM_CATEGORY_TOXICITY: 2>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>},\n",
            "                     {'category': <HarmCategory.HARM_CATEGORY_VIOLENCE: 3>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>},\n",
            "                     {'category': <HarmCategory.HARM_CATEGORY_SEXUAL: 4>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>},\n",
            "                     {'category': <HarmCategory.HARM_CATEGORY_MEDICAL: 5>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>},\n",
            "                     {'category': <HarmCategory.HARM_CATEGORY_DANGEROUS: 6>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>}]},\n",
            " {'output': 'There are 3 * 3 = 9 cats. Each cat has 4 mittens and a hat so '\n",
            "            \"that's 4 + 1 = 5 items per cat. So in total, 9 * 5 = 45 items \"\n",
            "            'were knit. The mittens take 7m each and there are 45 mittens so '\n",
            "            \"that's 7 * 45 = 315m of yarn used for mittens. The hats take 4m \"\n",
            "            \"each and there are 45 hats so that's 4 * 45 = 180m of yarn used \"\n",
            "            'for hats. In total, 315 + 180 = 495m of yarn was used.\\n'\n",
            "            'The answer: 495.',\n",
            "  'safety_ratings': [{'category': <HarmCategory.HARM_CATEGORY_DEROGATORY: 1>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>},\n",
            "                     {'category': <HarmCategory.HARM_CATEGORY_TOXICITY: 2>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>},\n",
            "                     {'category': <HarmCategory.HARM_CATEGORY_VIOLENCE: 3>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>},\n",
            "                     {'category': <HarmCategory.HARM_CATEGORY_SEXUAL: 4>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>},\n",
            "                     {'category': <HarmCategory.HARM_CATEGORY_MEDICAL: 5>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>},\n",
            "                     {'category': <HarmCategory.HARM_CATEGORY_DANGEROUS: 6>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>}]},\n",
            " {'output': 'Chain-of-thought:\\n'\n",
            "            'First find the total number of cats: 3 houses * 3 cats / house = '\n",
            "            '9 cats. Then multiply the number of cats by the number of mittens '\n",
            "            'per cat to find the total number of mittens: 9 cats * 4 mittens / '\n",
            "            'cat = 36 mittens. Then multiply the number of mittens by the '\n",
            "            'length of yarn per mitten to find the total length of yarn used '\n",
            "            'for mittens: 36 mittens * 7 m / mitten = 252 m. Then do the same '\n",
            "            'thing for hats: 9 cats * 1 hat / cat = 9 hats. Then multiply the '\n",
            "            'number of hats by the length of yarn per hat to find the total '\n",
            "            'length of yarn used for hats: 9 hats * 4 m / hat = 36 m. Then add '\n",
            "            'the lengths of yarn used for mittens and hats to find the total '\n",
            "            'length of yarn used: 252 m + 36 m = 288 m.\\n'\n",
            "            '\\n'\n",
            "            'The answer should be 288',\n",
            "  'safety_ratings': [{'category': <HarmCategory.HARM_CATEGORY_DEROGATORY: 1>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>},\n",
            "                     {'category': <HarmCategory.HARM_CATEGORY_TOXICITY: 2>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>},\n",
            "                     {'category': <HarmCategory.HARM_CATEGORY_VIOLENCE: 3>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>},\n",
            "                     {'category': <HarmCategory.HARM_CATEGORY_SEXUAL: 4>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>},\n",
            "                     {'category': <HarmCategory.HARM_CATEGORY_MEDICAL: 5>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>},\n",
            "                     {'category': <HarmCategory.HARM_CATEGORY_DANGEROUS: 6>,\n",
            "                      'probability': <HarmProbability.NEGLIGIBLE: 1>}]}]\n"
          ]
        }
      ],
      "source": [
        "import pprint\n",
        "pprint.pprint(completion.candidates)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5ad0836a2f6"
      },
      "source": [
        "So, since you know the answer to this problem, it's easy to check the solve rate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "xJh2LRHhxQ6k",
        "outputId": "44d08a7c-caed-4727-cac2-bc57e6392ff2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.625"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "import numpy as np\n",
        "np.mean(['288' in c['output'] for c in completion.candidates])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trying other types of promt"
      ],
      "metadata": {
        "id": "Xyp_1IrfgOrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tour_prompt = \"\"\"\n",
        "interesting places to travel to for the landscape and view.\n",
        "\"\"\"\n",
        "\n",
        "completion = palm.generate_text(\n",
        "    model=model,\n",
        "    prompt=tour_prompt,\n",
        "    temperature=1,\n",
        "    # The maximum length of the response\n",
        "    max_output_tokens=800,\n",
        ")\n",
        "\n",
        "print(completion.result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPYSgdZ0gOJU",
        "outputId": "3856e03e-daef-4c62-b6bf-2dfc6c26a201"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* **Yosemite National Park, California**. Yosemite is home to some of the most iconic landscapes in the United States, including towering granite cliffs, giant sequoia trees, and cascading waterfalls. The park is also a popular destination for hiking, camping, and rock climbing.\n",
            "* **The Grand Canyon, Arizona**. The Grand Canyon is one of the most visited natural wonders in the world, and for good reason. The canyon's steep cliffs and colorful rock formations are truly awe-inspiring. There are many ways to experience the Grand Canyon, including hiking, rafting, and mule rides.\n",
            "* **Machu Picchu, Peru**. Machu Picchu is a stunning Incan citadel that sits high in the Andes Mountains. The city was built in the 15th century and abandoned only a few decades later. Machu Picchu is a UNESCO World Heritage Site and one of the most popular tourist destinations in South America.\n",
            "* **The Great Wall of China**. The Great Wall of China is one of the most impressive man-made structures in the world. The wall stretches for over 6,000 kilometers and was built over a period of 2,000 years. The wall is a UNESCO World Heritage Site and a popular tourist destination.\n",
            "* **Taj Mahal, India**. The Taj Mahal is a beautiful white marble mausoleum that was built by Mughal emperor Shah Jahan in the 17th century. The tomb is dedicated to Shah Jahan's wife, Mumtaz Mahal. The Taj Mahal is a UNESCO World Heritage Site and one of the most popular tourist destinations in India.\n",
            "* **Kilimanjaro, Tanzania**. Kilimanjaro is the highest mountain in Africa and the highest free-standing mountain in the world. The mountain is a popular destination for mountaineers and hikers. The views from the summit are breathtaking, and you can see for miles in every direction.\n",
            "* **The Northern Lights**. The Northern Lights are a natural phenomenon that occurs when charged particles from the sun interact with the Earth's atmosphere. The lights are most visible in the Arctic and Antarctic regions, but they can also be seen in other parts of the world. The Northern Lights are a truly awe-inspiring sight, and they are one of the most popular tourist destinations in the world.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "tell me about you, when you were made, you last data trained on etc.\n",
        "\"\"\"\n",
        "\n",
        "completion = palm.generate_text(\n",
        "    model=model,\n",
        "    prompt=prompt,\n",
        "    temperature=0,\n",
        "    # The maximum length of the response\n",
        "    max_output_tokens=800,\n",
        ")\n",
        "\n",
        "print(completion.result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rr-nr0SgN7s",
        "outputId": "0c84e25c-c08c-474e-a24f-f49b144d91a1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I was trained on January 15, 2023. My data was last updated on February 25, 2023.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the previous prompt with a different temp"
      ],
      "metadata": {
        "id": "Hcr57AVThZJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "tell me about you, when you were made, you last data trained on etc.\n",
        "\"\"\"\n",
        "\n",
        "completion = palm.generate_text(\n",
        "    model=model,\n",
        "    prompt=prompt,\n",
        "    temperature=1,\n",
        "    # The maximum length of the response\n",
        "    max_output_tokens=800,\n",
        ")\n",
        "\n",
        "print(completion.result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4wIAKAuhSnp",
        "outputId": "1de44bdb-efa1-47b2-8f7c-105986cc4529"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am powered by PaLM 2, which stands for Pathways Language Model 2, a large language model from Google AI. I was trained in January 2023, and my last data was trained on March 8, 2023.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\" how to make your predictions better, things to do and steps to tailor it to specific use cases\n",
        "\"\"\"\n",
        "\n",
        "completion = palm.generate_text(\n",
        "    model=model,\n",
        "    prompt=prompt,\n",
        "    temperature=0,\n",
        "    # The maximum length of the response\n",
        "    max_output_tokens=800,\n",
        ")\n",
        "\n",
        "print(completion.result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZL9eR0nho8B",
        "outputId": "317de517-cba9-49db-fa07-81753984380b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are a number of things you can do to make your predictions better, including:\n",
            "\n",
            "* **Collect more data.** The more data you have, the more accurate your predictions will be. However, it's important to make sure that the data you collect is relevant to the task you're trying to predict.\n",
            "* **Clean your data.** Before you can use your data to train a model, you need to make sure that it's clean and free of errors. This means removing duplicate data points, dealing with missing values, and correcting any inconsistencies.\n",
            "* **Feature engineer your data.** Sometimes, the features you use to train your model can make a big difference in the accuracy of your predictions. Feature engineering is the process of transforming your data into features that are more relevant to the task you're trying to predict.\n",
            "* **Choose the right model.** There are many different machine learning models to choose from, and the right model for your task will depend on a number of factors, such as the amount of data you have, the complexity of the task, and the resources you have available.\n",
            "* **Tune your hyperparameters.** Hyperparameters are the parameters of your machine learning model that you can adjust to improve its performance. Tuning your hyperparameters can often make a big difference in the accuracy of your predictions.\n",
            "\n",
            "In addition to these general tips, there are also a number of things you can do to tailor your predictions to specific use cases. For example, if you're trying to predict customer churn, you might want to focus on features that are specific to your business, such as the number of products a customer has purchased or the length of time they've been a customer.\n",
            "\n",
            "Here are some specific steps you can take to tailor your predictions to specific use cases:\n",
            "\n",
            "1. **Identify the key features for your task.** What are the features that are most relevant to the task you're trying to predict? For example, if you're trying to predict customer churn, you might want to focus on features such as the number of products a customer has purchased, the length of time they've been a customer, and their spending habits.\n",
            "2. **Clean your data.** Make sure that your data is clean and free of errors. This means removing duplicate data points, dealing with missing values, and correcting any inconsistencies.\n",
            "3. **Feature engineer your data.** Transform your data into features that are more relevant to the task you're trying to predict. For example, you might create a feature that represents the number of days since a customer last purchased a product.\n",
            "4. **Choose the right model.** There are many different machine learning models to choose from, and the right model for your task will depend on a number of factors, such as the amount of data you have, the complexity of the task, and the resources you have available.\n",
            "5. **Tune your hyperparameters.** Adjust the hyperparameters of your model to improve its performance. This can often make a big difference in the accuracy of your predictions.\n",
            "6. **Evaluate your predictions.** Once you've trained your model, you need to evaluate its performance. This can be done by using a holdout set of data or by using cross-validation.\n",
            "\n",
            "By following these steps, you can make your predictions better and more tailored to your specific use case.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "in 7 words, explain the term flow state\n",
        "\"\"\"\n",
        "\n",
        "completion = palm.generate_text(\n",
        "    model=model,\n",
        "    prompt=prompt,\n",
        "    temperature=0,\n",
        "    # The maximum length of the response\n",
        "    max_output_tokens=800,\n",
        ")\n",
        "\n",
        "print(completion.result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBJ5CfxNiZzt",
        "outputId": "8368a836-ee52-45e5-d3b3-1f6b0e7be0f6"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Being fully immersed in an activity\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "write a 5 lines rhyme on the grace\n",
        "\"\"\"\n",
        "\n",
        "completion = palm.generate_text(\n",
        "    model=model,\n",
        "    prompt=prompt,\n",
        "    temperature=0,\n",
        "    # The maximum length of the response\n",
        "    max_output_tokens=800,\n",
        ")\n",
        "\n",
        "print(completion.result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGEQa_U1jPf3",
        "outputId": "64f97ea9-c78c-4848-ac1c-06ee080c0aaf"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grace is a gift from God,\n",
            "A blessing to behold.\n",
            "It's a feeling of peace and joy,\n",
            "That fills our hearts with love.\n",
            "It's a strength that helps us through,\n",
            "The tough times in our lives.\n",
            "Grace is a gift that never fades,\n",
            "It's a part of us, always.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "steps to make PaLM 2 model generate better outputs or results\n",
        "\"\"\"\n",
        "\n",
        "completion = palm.generate_text(\n",
        "    model=model,\n",
        "    prompt=prompt,\n",
        "    temperature=0,\n",
        "    # The maximum length of the response\n",
        "    max_output_tokens=800,\n",
        ")\n",
        "\n",
        "print(completion.result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-fWN-2-lZML",
        "outputId": "22aa3c5d-206f-47a9-d998-a573d4eaf4f1"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. **Use more data.** The more data a model is trained on, the more it will learn and the better its results will be. PaLM 2 was trained on a massive dataset of text, but you can still improve its performance by giving it access to more data. You can do this by providing it with additional text files, websites, or other sources of data.\n",
            "2. **Fine-tune the model.** Once a model has been trained on a large dataset, you can further improve its performance by fine-tuning it on a specific task. This involves training the model on a dataset of data that is relevant to the task you want it to perform. For example, if you want to use PaLM 2 to generate text, you could fine-tune it on a dataset of text data.\n",
            "3. **Use a different architecture.** The architecture of a model refers to its underlying structure. Different architectures are better suited for different tasks. For example, Transformer-based models like PaLM 2 are well-suited for natural language processing tasks, but they may not be as good at other tasks, such as image recognition. If you are not satisfied with the results you are getting from PaLM 2, you can try using a different architecture.\n",
            "4. **Use a different pre-trained model.** PaLM 2 is a large language model that was pre-trained on a massive dataset of text. However, there are other large language models available, and you may find that one of them works better for your specific task. For example, if you are working on a task that requires knowledge of the real world, you may want to use a model that was pre-trained on a dataset of factual information.\n",
            "5. **Use a different set of parameters.** The parameters of a model are the values that control its behavior. By changing the parameters of a model, you can change its performance. For example, you can increase the size of the model's vocabulary or the number of layers in its neural network. This can improve the model's performance on some tasks, but it may also make it slower or more difficult to train.\n",
            "6. **Use a different training procedure.** The training procedure for a model refers to the way in which it is trained. Different training procedures can produce different results. For example, you can use a supervised training procedure, in which the model is trained on a dataset of labeled data, or an unsupervised training procedure, in which the model is trained on a dataset of unlabeled data. The training procedure you choose will depend on the task you are trying to achieve.\n",
            "7. **Use a different set of hyperparameters.** The hyperparameters of a model are the values that control its training process. By changing the hyperparameters of a model, you can change its performance. For example, you can increase the learning rate or the number of epochs. This can improve the model's performance on some tasks, but it may also make it slower or more difficult to train.\n",
            "8. **Use a different software library.** The software library you use to train and use a model can make a big difference in its performance. Some libraries are more efficient than others, and some libraries offer more features. If you are not satisfied with the performance you are getting from PaLM 2, you can try using a different software library.\n",
            "9. **Use a different hardware configuration.** The hardware configuration you use to train and use a model can also make a big difference in its performance. Some hardware configurations are more powerful than others, and some hardware configurations offer more memory. If you are not satisfied with the performance you are getting from PaLM 2, you can try using a different hardware configuration.\n",
            "10. **Use a combination of these techniques.** The best way to improve the performance of PaLM 2 is to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "how many parameters does paLM 2 have and what are they, which ones are adjustable\n",
        "\"\"\"\n",
        "\n",
        "completion = palm.generate_text(\n",
        "    model=model,\n",
        "    prompt=prompt,\n",
        "    temperature=0,\n",
        "    # The maximum length of the response\n",
        "    max_output_tokens=800,\n",
        ")\n",
        "\n",
        "print(completion.result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88mqg41jma95",
        "outputId": "57ffebb9-beef-472a-fcd2-b32d34781f14"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "paLM 2 has 540 billion parameters. These parameters are divided into two parts:\n",
            "\n",
            "* **Embedding parameters:** These parameters are used to represent words, phrases, and sentences. They are adjustable.\n",
            "* **Transformer parameters:** These parameters are used to learn the relationships between words and phrases. They are not adjustable.\n",
            "\n",
            "The number of adjustable parameters in paLM 2 is 537 billion.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "write a code to adjust some of paLM 2 parameters\n",
        "\"\"\"\n",
        "\n",
        "completion = palm.generate_text(\n",
        "    model=model,\n",
        "    prompt=prompt,\n",
        "    temperature=0,\n",
        "    # The maximum length of the response\n",
        "    max_output_tokens=800,\n",
        ")\n",
        "\n",
        "print(completion.result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSPrgIRVm0BH",
        "outputId": "aaed1893-35c9-47ca-c3c4-2b89b341b7e1"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```python\n",
            "import palm2\n",
            "\n",
            "# Initialize a palm2 model\n",
            "model = palm2.Palm2Model.from_pretrained(\"palm2-large\")\n",
            "\n",
            "# Adjust the parameters\n",
            "model.config.num_beams = 5\n",
            "model.config.temperature = 0.9\n",
            "model.config.top_k = 10\n",
            "\n",
            "# Print the adjusted parameters\n",
            "print(model.config)\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "write a code to adjust 20 different paLM 2 parameters\n",
        "\"\"\"\n",
        "\n",
        "completion = palm.generate_text(\n",
        "    model=model,\n",
        "    prompt=prompt,\n",
        "    temperature=0,\n",
        "    # The maximum length of the response\n",
        "    max_output_tokens=800,\n",
        ")\n",
        "\n",
        "print(completion.result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxeqfFZqnMcV",
        "outputId": "da86b241-9c3b-4360-bbe9-9ba2b1e9c55a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```python\n",
            "import palm2\n",
            "\n",
            "# Define the parameters to adjust\n",
            "parameters = {\n",
            "    \"num_layers\": [1, 2, 3, 4, 5],\n",
            "    \"hidden_size\": [128, 256, 512, 1024],\n",
            "    \"num_heads\": [4, 8, 16, 32],\n",
            "    \"attention_dropout\": [0.0, 0.1, 0.2, 0.3],\n",
            "    \"relu_dropout\": [0.0, 0.1, 0.2, 0.3],\n",
            "    \"layer_norm_epsilon\": [1e-5, 1e-6, 1e-7, 1e-8],\n",
            "}\n",
            "\n",
            "# Create a list of models, each with a different set of parameters\n",
            "models = []\n",
            "for p in parameters:\n",
            "    for v in parameters[p]:\n",
            "        model = palm2.Palm2(\n",
            "            num_layers=p,\n",
            "            hidden_size=v,\n",
            "            num_heads=v,\n",
            "            attention_dropout=v,\n",
            "            relu_dropout=v,\n",
            "            layer_norm_epsilon=v,\n",
            "        )\n",
            "        models.append(model)\n",
            "\n",
            "# Evaluate each model on a test set\n",
            "results = []\n",
            "for model in models:\n",
            "    results.append(model.evaluate(test_set))\n",
            "\n",
            "# Print the results\n",
            "print(results)\n",
            "```\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}